# -*- coding: utf-8 -*-
"""mancala_qin_leclair_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16sxzTQtEeVK2fmkryLGSXSwyaMsSLjDw

# **Imports Necessary**
"""

!pip install keras-rl2
!pip install tensorflow==2.3.0
import gym
from gym import spaces
import random
import numpy as np
from collections import namedtuple
from itertools import count
import math
import matplotlib.pyplot as plt
from statistics import mean
from google.colab import files

# Imports for tensorflow

from tensorflow.keras.models import Sequential #toggle this 
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.optimizers import Adam

from keras.callbacks import TensorBoard

from rl.agents import DQNAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory

import tensorflow as tf
np.random.seed(0)
random.seed(0)
tf.random.set_seed(0)

"""# **Defining the necessary components for environment**"""

PLAYER1_TURN = 0
PLAYER2_TURN = 1
class Mancala:
  def __init__(self):
    # Set to true if game is over
    self.is_over = False

    # This would be the board, we do not include the stores
    self.board = np.array([[4,4,4,4,4,4,0], [4,4,4,4,4,4,0]])
    # self.board = np.array([[0,0,0,0,0,1,0], [1,4,4,4,4,4,0]])
    # Initializing whose turn it is
    self.turn = PLAYER1_TURN

    # This is information for rewards
    self.turn_again = False
    self.stole = False
    self.turn_score = 0

    # Winner is -1 if no winner yet, 0 if player 1, and 1 if player 2
    self.winner = -1

  def reset(self):
    
    # Reset the board, we do not include the stores
    self.board = np.array([[4,4,4,4,4,4,0], [4,4,4,4,4,4,0]])

    # reset the game
    self.is_over = False
    
    # Initializing whose turn it is
    self.turn = PLAYER1_TURN

    # This is information for rewards
    self.turn_again = False
    self.stole = False
    self.turn_score = 0

    self.winner = -1
  
  # Gets the possible actions
  def actions_possible(self):
    actions = []
    index = 0
    for pocket in self.board[self.turn, :]:
      if index == 6:
        break
      if pocket != 0:
        actions.append(index)
      index += 1
    return actions

  def is_valid_move(self, pocket):
    if self.board[self.turn, pocket] == 0:
      if self.turn == PLAYER1_TURN:
        self.winner = PLAYER2_TURN
      return False
    return True

  def move(self, pocket):
    # Resetting rewards for current turn
    self.turn_again = False
    self.stole = False
    self.turn_score = 0

    # Getting the number of stones in pocket
    num_stones = self.board[self.turn, pocket]
    self.board[self.turn, pocket] = 0

    # While loop places stones in appropriate pocket
    side = self.turn
    while num_stones != 0:
      if pocket == 6:
        side = 1 - side
        pocket = -1

      pocket += 1

      if (pocket == 6 and side == self.turn) or pocket != 6:
        num_stones -= 1  
        self.board[side, pocket] += 1
        # We add to our current turn score
        if pocket == 6 and side == self.turn:
          self.turn_score += 1

    # Handling if this player gets a second turn
    if pocket == 6 and side == self.turn:
      self.turn_again = True

    # Checking if the player stole a piece
    if side == self.turn and self.board[self.turn, pocket] == 1 and pocket != 6 and self.board[1 - self.turn, 5 - pocket] != 0:
      self.turn_score += self.board[1 - self.turn, 5 - pocket] + self.board[self.turn, pocket]
      self.board[self.turn, 6] += self.board[1 - self.turn, 5 - pocket] + self.board[self.turn, pocket]
      self.board[self.turn, pocket] = 0
      self.board[1 - self.turn, 5 - pocket] = 0
      self.stole = True

    # Switching turn if necessary
    if not self.turn_again:
      self.turn = 1 - self.turn

    if np.sum(self.board[0,0:6]) == 0 or np.sum(self.board[1,0:6]) == 0:
      self.is_over = True;
      self.clear_lanes()
      if self.board[PLAYER1_TURN, 6] > self.board[PLAYER2_TURN, 6]:
        self.winner = 0
      elif self.board[PLAYER1_TURN, 6] < self.board[PLAYER2_TURN, 6]:
        self.winner = 1

  def get_num_stones(self, side):
    return np.sum(self.board[side,0:6])

  def check_if_over(self):
    return self.is_over

  def clear_lanes(self):
    for pocket in range(6):
      curr_p1 = self.board[0,pocket]
      curr_p2 = self.board[1,pocket]
      self.board[0,6] += curr_p1
      self.board[1,6] += curr_p2
      self.board[0,pocket] = 0
      self.board[1,pocket] = 0
  
  def get_turn_info(self):
    current_info = {
        "turn" : self.turn,
        "turn_score" : self.turn_score,
        "turn_again" : self.turn_again,
        "turn_stole" : self.stole,
        "is_over" : self.is_over        
    }
    return current_info
  
  # Returns the state of the board
  def get_board_state(self):
    return self.board

  # Shows board state
  def show_board(self):
    print("---------------------")
    print(" " + str(np.flip(self.board[1,0:6])))
    print(str(self.board[1,6]) + '             ' + str(self.board[0,6]))
    print(" " + str(self.board[0,0:6]))
    if self.is_over:
      if self.winner == 0:
        print("Player 1 won!")
      elif self.winner == 1:
        print("Player 2 won!")
      else:
        print("It's a tie!")

# A bot we created to facilitate training, we can adjust it's priorities
class SmartBot():
  '''
  Priorities are as follows"
  - Extra turn
  - Steal
  - Hoard: If we want it to hoard pieces on it's side
  - Flight: Prevent the other player from stealing if there are more than 3 stones
  Otherwise, take the highest value action, check every pit and do the one with highest score
  at random
  '''
  def __init__(self, priorities):
    self.priorities = priorities

  def get_biggest_steal(self, side, board):
    curr_board = board
    if (side == 1):
      curr_board = np.array([board[1,:],board[0,:]])
    # Case 1
    index = 0
    maximum_steal = (0,0)
    for stones in curr_board[0,:]:
      if stones == 13:
        if (curr_board[1, 5 - index] + 3) > maximum_steal[1]:
          maximum_steal = (index, (curr_board[1, 5 - index] + 3))
      index += 1

    # Case 2
    index_1 = 0
    for stones in curr_board[0,:]:
      if stones == 0 and curr_board[1, 5 - index_1] != 0:    
        index_2 = 0        
        for stones_pos in curr_board[0,:]:
          if (index_2 < index_1 and (stones_pos + index_2) == index_1):
            if (curr_board[1, 5 - index_1] + 1) > maximum_steal[1]:
              maximum_steal = (index_2, (curr_board[1, 5 - index_1] + 1))
          if (index_2 > index_1 and (stones_pos + index_2 - 13) == index_1):
            if (curr_board[1, 5 - index_1] + 3) > maximum_steal[1]:
              maximum_steal = (index_2, (curr_board[1, 5 - index_1] + 3))
          index_2 += 1
      index_1 += 1
    return maximum_steal

  def choose_action(self, board):
    curr_board = np.array([board[1,0:6],board[0,0:6]])
    for p in self.priorities:
      if p[0] == 'E' and random.random() < p[1]:
        index = 5
        for stones in curr_board[0,::-1]:
          if stones == (6 - index) or (stones + index) == 19:
            return index
          index -= 1
      if p[0] == 'S' and random.random() < p[1]:
        result = self.get_biggest_steal(0, curr_board)
        if result[1] != 0:
          return result[0]
      if p[0] == 'H' and random.random() < p[1]:
        index = 0
        for stones in curr_board[0,:]:
          index = 5
          for stones in curr_board[0,::-1]:
            if stones < 6 - index and stones != 0:
              return index
            index -= 1
      if p[0] == 'F' and random.random() < p[1]:
        result = self.get_biggest_steal(1, curr_board)
        if result[1] != 0:
          stealing_slot = curr_board[1,result[0]] + result[0] 
          while stealing_slot >= 13:
            stealing_slot -= 13
          return 5 - stealing_slot
    index = 0
    for stones in curr_board[0,:]:
      index = 5
      for stones in curr_board[0,::-1]:
        if stones >= 6 - index and stones != 0:
          return index
        index -= 1
    return 0

class PlotMetric():
  def __init__(self, name, granularity):
    self.plot_data = []
    self.running_data = []
    self.amount = 0.0
    self.granularity = granularity
    self.name = name
  
  def update_amount(self, amount):
    self.amount += float(amount)
  
  def game_end(self):
    self.running_data.append(self.amount)
    if len(self.running_data) == self.granularity:
      self.plot_data.append(mean(self.running_data))
      self.running_data = []
    self.amount = 0.0

  def plot(self):
    plt.xlabel('Game number (' + str(self.granularity) + ')')
    plt.ylabel(self.name)
    plt.title(self.name)
    plt.grid(b=True, alpha=0.5)
    plt.plot(self.plot_data)
    plt.show()

class MancalaEnv(gym.Env):
  def __init__(self):

    # Current step
    self.current_step = 0

    # The type of player 2 
    self.p2_type = p2_type

    # This would be the rewards
    self.reward_range = (MINIMUM_REWARD, MAXIMUM_REWARD)

    # There are at most four actions we can take every turn
    self.action_space = spaces.Discrete(6)

    # Initializing a mancala board
    self.board = Mancala()

    # Creating the variable for an ML opponent if it exists
    self.ml_opponent = None

    # Hard bot
    self.bot = SmartBot(smart_bot_priorities)

    # This would be the board, we do not include the stores
    self.observation_space = self.board.get_board_state()

    # This is for plotting

    self.stole_metric = PlotMetric('Steals Per Game By Agent', 10)
    self.stolefrom_metric = PlotMetric('Steals Per Game by Opponent', 10)
    self.extra_metric = PlotMetric('Extra turns per game', 10)
    self.length_metric = PlotMetric('Length of games', 10)
    self.invalid_metric = 0
    self.game_num = 0
    self.invalid_metric_list = []

  def reset(self):
    self.game_num += 1
    self.length_metric.update_amount(self.current_step)

    if self.game_num % 10 == 0:
      self.invalid_metric_list.append(self.invalid_metric)

    self.current_step = 0

    self.stole_metric.game_end()
    self.stolefrom_metric.game_end()
    self.extra_metric.game_end()
    self.length_metric.game_end()

    # Reset the mancala board
    self.board.reset()
    self.observation_space = self.board.get_board_state()

    return self.observation_space

  # Gets the action for the second player
  def get_action_p2(self):
    if self.p2_type == RANDOM_PLAYER:
      possible_actions = self.board.actions_possible()
      return possible_actions[random.randrange(len(possible_actions))]
    if self.p2_type == USER_PLAYER:
      possible_actions = self.board.actions_possible()
      opponent_move = input("Please input a number from 0-5: ")
      while not opponent_move.isdigit() or int(opponent_move) not in possible_actions:
          opponent_move = input("Wrong input, please try again: ")
      return int(opponent_move)
    if self.p2_type == ML_PLAYER:
      possible_actions = self.board.actions_possible()
      ml_pick = self.ml_opponent.forward(np.array([self.board.get_board_state()[1,:],self.board.get_board_state()[0,:]]))
      if random.random() < opponent_agent_epsilon or ml_pick not in possible_actions:
        return possible_actions[random.randrange(len(possible_actions))]
      return ml_pick
    if self.p2_type == BOT_PLAYER:
      possible_actions = self.board.actions_possible()
      bot_pick = self.bot.choose_action(self.board.get_board_state())
      if random.random() < opponent_bot_epsilon or bot_pick not in possible_actions:
        return possible_actions[random.randrange(len(possible_actions))]
      return bot_pick
      
  def set_mlagent(self, agent):
    if self.p2_type == ML_PLAYER:
      self.ml_opponent = agent
    else:
      print("You should not be calling this with this type of player: ML AGENT")

  # Calculate reward for either moves made by player 1 or player 2
  def calculate_reward(self, player, move_result):
    game_advancement = self.current_step / average_num_steps
    num_stones_on_side = self.board.get_num_stones(player)
    turn_score = move_result["turn_score"]
    turn_again = int(move_result["turn_again"] == True)
    turn_stole = int(move_result["turn_stole"] == True) 
    game_over = int(move_result["is_over"] == True)

    reward = 0.0

    if game_over == 1:
      reward += (1 - self.board.winner) * reward_winning + self.board.winner * reward_losing  

    if player == PLAYER1_TURN:
      self.extra_metric.update_amount(turn_again)
      self.stole_metric.update_amount(turn_stole)
      if simple_score:
        return turn_score
      reward = tune_score_p1 * turn_score + tune_again_p1 * turn_again + tune_stole_p1 * turn_stole
      reward += num_stones_on_side * tune_num_stones_p1 * game_advancement
      if game_over == 1 and self.board.winner == PLAYER1_TURN:
        reward += reward_winning
    if player == PLAYER2_TURN:
      self.stolefrom_metric.update_amount(turn_stole)
      if simple_score:
        return -turn_score
      reward = tune_score_p2 * turn_score + tune_again_p2 * turn_again + tune_stole_p2 * turn_stole
      reward += num_stones_on_side * tune_num_stones_p2 * game_advancement
      if game_over == 1 and self.board.winner == PLAYER2_TURN:
        reward += reward_losing
    return reward
      

  def step(self, action):
    self.current_step += 1

    # Initializing rewards 
    step_reward = 0.0
    
    # Check if move is invalid, if so, set negative reward and end game
    if not self.board.is_valid_move(action):
      self.invalid_metric += 1
      step_reward += invalid_move_reward
      return self.board.get_board_state(), step_reward, True, {}        

    # Do an action
    self.board.move(action)

    if self.p2_type == USER_PLAYER:
      self.render()

    # Get the results of the current action
    results_p1 = self.board.get_turn_info()

    # Calculate reward for first player move
    step_reward += self.calculate_reward(PLAYER1_TURN, results_p1)
    
    curr_turn = results_p1["turn"]
    # The second player's turn
    while curr_turn == PLAYER2_TURN and not self.board.check_if_over():
      # Have the second player make a move
      self.board.move(self.get_action_p2())

      if self.p2_type == USER_PLAYER:
        self.render()

      results_p2 = self.board.get_turn_info()

      # Calculate reward for second player move
      step_reward += self.calculate_reward(PLAYER2_TURN, results_p2)

      curr_turn = results_p2["turn"]

    return self.board.get_board_state(), step_reward, self.board.check_if_over(), {}

  def plot_info(self):
    self.stole_metric.plot()
    self.stolefrom_metric.plot()
    self.extra_metric.plot()
    self.length_metric.plot()
    plt.ylabel('Invalid Game Numbers')
    plt.title('Invalid Game Numbers')
    plt.grid(b=True, alpha=0.5)
    plt.plot(self.invalid_metric_list)
    plt.show()

  def render(self, mode = 'human', close = False):
    self.board.show_board()

def build_model(states, actions):
    model = Sequential()
    model.add(Flatten(input_shape=(1,2,7)))
    print(states)
    print(model.output_shape)
    model.add(Dense(14, activation='relu'))
    model.add(Dense(48, activation='relu'))
    model.add(Dense(48, activation='relu'))
    model.add(Dropout(0.2, input_shape=(48,)))
    model.add(Dense(48, activation='relu'))
    model.add(Dense(14, activation='relu'))
    model.add(Dense(actions, activation='linear'))
    return model

def build_agent(model, actions):
    policy = BoltzmannQPolicy(tau=tau_model)
    memory = SequentialMemory(limit=50000, window_length=1)
    dqn = DQNAgent(model=model, memory=memory, policy=policy, enable_double_dqn=True, 
                  nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-2)
    return dqn

"""# **Preparing for Training**"""

# Parameters for Mancala Environment
MINIMUM_REWARD = -1000
MAXIMUM_REWARD = 1000

RANDOM_PLAYER = 0 # This kind of player will choose an action at random
USER_PLAYER = 1 # This is a human-controlled opponet, i.e us
ML_PLAYER = 2 # This is another agent
BOT_PLAYER = 3 # This is an agent that is smarter than random

# Change this to vary what player we are playing against
p2_type = BOT_PLAYER

MAX_STEPS = 1000

# Variables to tune rewards
tune_score_p1 = 1.0
tune_again_p1 = 0.0
tune_stole_p1 = 10.0
tune_score_p2 = -1.0
tune_again_p2 = -0.0
tune_stole_p2 = -5.0
tune_num_stones_p1 = 0.0
tune_num_stones_p2 = -0.0
invalid_move_reward = -50.0   
reward_winning = 0.0
reward_losing = 0.0

average_num_steps = 25.0

# Priorities for smart bot
smart_bot_priorities = [('E',0.9), ('S', 0.9), ('F',0.15), ('H',0.6)]
#smart_bot_priorities = ['E', 'S','F', 'H']


simple_score = False # if set, will only reward based on points scored

# variable to pick random action when playing against another agent
opponent_agent_epsilon = 0.5
opponent_bot_epsilon = 0.5

# How much it explores vs exploits
tau_model = 2.0

env = MancalaEnv()
env.reset()
states = len(env.observation_space[0]) + len(env.observation_space[1])
actions = env.action_space.n

# Build model
model = build_model(states, actions)

# Loading previous model, if specified ML_PLAYER
p2_ml = None
if env.p2_type == ML_PLAYER:
  location = 'random_v3.h5f'
  p2_ml = build_agent(model, actions)
  p2_ml.compile(Adam(lr=1e-3), metrics=['mae'])
  p2_ml.load_weights(location)
  env.set_mlagent(p2_ml)

# See summary of the model
model.summary()

"""# **Training**"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

tb = TensorBoard(log_dir='/keras-rl')

!rm -rf /keras-rl

dqn = build_agent(model, actions)
dqn.compile(Adam(lr=1e-3), metrics=['mae'])
dqn.fit(env, nb_steps=400000, visualize=False, verbose=1, callbacks=[tb])

# Commented out IPython magic to ensure Python compatibility.
# Plotting
# %tensorboard --logdir "/keras-rl"
env.plot_info()

"""# **Saving Model**"""

name_file = 'megatron_steal_v1.h5f' # need to add .h5f at the end
dqn.save_weights(name_file, overwrite=True)

files.download(name_file + ".index")
files.download(name_file + ".data-00000-of-00001")

"""# **Loading Model**"""

env_test = MancalaEnv()
env_test.reset()
states_new = len(env_test.observation_space[0]) + len(env_test.observation_space[1])
actions_new = env_test.action_space.n

# Build model
model_new = build_model(states_new, actions_new)

name_file = 'megatron_natural_v2.h5f'
dqn = build_agent(model_new, actions_new)
dqn.compile(Adam(lr=1e-3), metrics=['mae'])
dqn.load_weights(name_file)

"""# **Evaluating**"""

# Choose who to evaluate against
p2_type = USER_PLAYER

# We want to evaluate with a simple_score
simple_score = True

env_test = MancalaEnv()
env_test.reset()
states_new = len(env_test.observation_space[0]) + len(env_test.observation_space[1])
actions_new = env_test.action_space.n

# Build model
model_new = build_model(states_new, actions_new)

# Loading previous model, if specified ML_PLAYER
p2_ml = None
if env_test.p2_type == ML_PLAYER:
  location = 'random_v1.h5f'
  p2_ml = build_agent(model_new, actions_new)
  p2_ml.compile(Adam(lr=1e-3), metrics=['mae'])
  p2_ml.load_weights(location)
  env_test.set_mlagent(dqn)

scores = dqn.test(env_test, nb_episodes=10, visualize=False) # Visualize should be true if p2_type is user
print(np.mean(scores.history['episode_reward']))

